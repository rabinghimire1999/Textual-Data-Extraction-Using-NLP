{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0eaBXLn29Ws_"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Read the input file\n",
        "df = pd.read_excel('/content/Input.xlsx')\n",
        "\n",
        "# Loop through each URL and extract the article text\n",
        "for index, row in df.iterrows():\n",
        "    url = row['URL']\n",
        "    url_id = row['URL_ID']\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    # Extract the article title\n",
        "    article_title = soup.title.string\n",
        "    # Extract the article text\n",
        "    article_text = ''\n",
        "    article_tags = soup.find_all('p')\n",
        "    for tag in article_tags:\n",
        "        article_text += tag.get_text()\n",
        "    # Save the extracted article text in a text file\n",
        "    with open(f'{url_id}.txt', 'w') as f:\n",
        "        f.write(article_title + '\\n' + article_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textstat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5G3et5H_wJj",
        "outputId": "e1e8e7d9-f8d4-4b80-9995-eac4ccdc7690"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: textstat in /usr/local/lib/python3.8/dist-packages (0.7.3)\n",
            "Requirement already satisfied: pyphen in /usr/local/lib/python3.8/dist-packages (from textstat) (0.13.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Analysis\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from textstat import flesch_reading_ease, textstat\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Read the output structure file\n",
        "output_df = pd.read_excel('/content/Output Data Structure.xlsx')\n",
        "\n",
        "# Loop through each URL and extract the article text\n",
        "for index, row in output_df.iterrows():\n",
        "    url_id = row['URL_ID']\n",
        "    with open(f'{url_id}.txt', 'r') as f:\n",
        "        text = f.read()\n",
        "    # Extract the article title and article text\n",
        "    article_title = text.split('\\n')[0]\n",
        "    article_text = ' '.join(text.split('\\n')[1:])\n",
        "    # Perform textual analysis on the article text\n",
        "    sia = SentimentIntensityAnalyzer()\n",
        "    sentiment = sia.polarity_scores(article_text)\n",
        "    positive_score = sentiment['pos']\n",
        "    negative_score = sentiment['neg']\n",
        "    polarity_score = sentiment['compound']\n",
        "    subjectivity_score = 1 - polarity_score\n",
        "    sentences = nltk.sent_tokenize(article_text)\n",
        "    sentence_lengths = [len(nltk.word_tokenize(sentence)) for sentence in sentences]\n",
        "    if sentence_lengths:\n",
        "        avg_sentence_length = sum(sentence_lengths) / len(sentence_lengths)\n",
        "    else:\n",
        "        avg_sentence_length = 0\n",
        "    words = nltk.word_tokenize(article_text)\n",
        "    word_count = len(words)\n",
        "    if word_count:\n",
        "        complex_words = [word for word in words if textstat.syllable_count(word) > 2]\n",
        "        complex_word_count = len(complex_words)\n",
        "        percentage_complex_words = complex_word_count / word_count * 100\n",
        "    else:\n",
        "        complex_word_count = 0\n",
        "        percentage_complex_words = 0\n",
        "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
        "    syllable_count = textstat.syllable_count(article_text)\n",
        "    if len(sentence_lengths) > 0:\n",
        "      avg_words_per_sentence = word_count / len(sentence_lengths)\n",
        "    else:\n",
        "      avg_words_per_sentence = 0\n",
        "    personal_pronouns = sum([1 for word in nltk.pos_tag(words) if word[1] == 'PRP'])\n",
        "    if word_count > 0:\n",
        "      avg_word_length = sum(len(word) for word in words) / word_count\n",
        "    else:\n",
        "      avg_word_length = 0\n",
        "    # Compute the required variables based on the analysis\n",
        "    output_df.loc[index, 'Positive Score'] = positive_score\n",
        "    output_df.loc[index, 'Negative Score'] = negative_score\n",
        "    output_df.loc[index, 'Polarity Score'] = polarity_score\n",
        "    output_df.loc[index, 'Subjectivity Score'] = subjectivity_score\n",
        "    output_df.loc[index, 'Avg Sentence Length'] = avg_sentence_length\n",
        "    output_df.loc[index, 'Percentage of Complex Words'] = percentage_complex_words\n",
        "    output_df.loc[index, 'FOG Index'] = fog_index\n",
        "    output_df.loc[index, 'Complex Word Count'] = complex_word_count\n",
        "    output_df.loc[index, 'Word Count'] = word_count\n",
        "    if word_count > 0:\n",
        "      output_df.loc[index, 'Syllable per Word'] = syllable_count / word_count\n",
        "    else:\n",
        "      output_df.loc[index, 'Syllable per Word'] = 0\n",
        "    output_df.loc[index, 'Personal Pronouns'] = personal_pronouns\n",
        "    output_df.loc[index, 'Avg Word Length'] = avg_word_length\n",
        "    output_df.to_excel('/content/Output Data Structure.xlsx', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoq2vOjO9yby",
        "outputId": "384e2926-70d1-4bcf-d1be-d2e07601bee8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k8SJn1mMT7SF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}